############################################################
# EXPERIMENTO BERT-PLI - Configuração 4
# Objetivo: Recuperação de Casos Jurídicos em Nível de Parágrafo
#
# Estratégia:
# - BertAdam (otimizador especializado para BERT)
# - LR conservador para fine-tuning cuidadoso
# - Batch médio balanceando eficiência e regularização
# - Poucas épocas (evitar overfitting em tarefas jurídicas)
############################################################

############################
# METADADOS DO EXPERIMENTO
############################
[experiment]
name = bertpli4
# name = bertpli_bertadam_lr5e5_bs24_ep3

description = |
    BertAdam implementa correção de bias específica para BERT.
    LR 5e-5 comum em papers de fine-tuning BERT.
    3 épocas frequentemente suficientes para convergência em legal NLP.
    Batch 24 otimiza uso de GPU mantendo estabilidade.
seed = 42

############################
# AMBIENTE DE EXECUÇÃO
############################
[environment]
precision = fp16
notes = |
    BertAdam ajusta automaticamente momentos para warmup.
    Configuração equilibrada para GPU mid-range (~12-16GB).

############################
# DADOS
############################
[data]
data_dir = data/
max_paragraphs = 64
max_seq_length = 256
train_dataset_type = JsonFromFiles
train_formatter_type = BertPairText
train_data_path = data
train_file_list = train_task2.json
valid_dataset_type = JsonFromFiles
valid_formatter_type = BertPairText
valid_data_path = data
valid_file_list = valid_task2.json
test_dataset_type = JsonFromFiles
test_formatter_type = BertPairText
test_data_path = data
test_file_list = test_task1.json
recursive = False
json_format = line
load_into_mem = True

############################
# MODELO
############################
[model]
pretrained_model = bert-base-uncased
aggregation_model = atten_rnn
hidden_size = 256
dropout = 0.1
freeze_bert = false
freeze_layers = 0
model_name = BertPoint
bert_path = bert-base-uncased
output_dim = 2
output_mode = classification

############################
# TREINAMENTO
############################
[train]
epoch = 3
batch_size = 24
shuffle = true
gradient_accumulation_steps = 1
optimizer = bert_adam
learning_rate = 5e-5
weight_decay = 0.01
max_grad_norm = 1.0
warmup_ratio = 0.1
lr_scheduler = linear
early_stopping = true
early_stopping_patience = 2
reader_num = 0
step_size = 1
lr_multiplier = 1

############################
# AVALIAÇÃO
############################
[eval]
metrics = precision, recall, f1
eval_strategy = epoch
monitor_metric = f1
monitor_mode = max
# pool_out=True: estágio 1 (extrator de embeddings). Avaliação de recuperação é do AttenRNN.
run_test_at_end = false
eval_batch_size = 12
eval_reader_num = 0

############################
# LOGGING & OUTPUT
############################
[logging]
output_dir = output/experiment
save_checkpoints = true
save_strategy = epoch
log_loss = true
log_metrics = true
logger = tensorboard

############################
# OUTPUT
############################
[output]
output_time = 1
test_time = 1
model_path = output/checkpoints
model_name = bert_finetuned
pool_out = True
save_as_dict = True
tensorboard_path = output/tensorboard
accuracy_method = SingleLabelTop1
output_function = Basic
output_value = micro_precision,macro_precision,macro_recall,macro_f1
tqdm_ncols = 150

############################
# MONITORAMENTO DE RECURSOS
############################
[monitoring]
enable_monitoring = true
collect_energy = true
collect_ram = true
collect_flops = true
collect_time = true
energy_tool = codecarbon
ram_tool = psutil
flops_tool = torch_profiler
monitoring_granularity = epoch
metrics_output_dir = outputs/metrics/
save_summary = true