############################################################
# EXPERIMENTO BERT-PLI - Configuração 3
# Objetivo: Recuperação de Casos Jurídicos em Nível de Parágrafo
#
# Estratégia:
# - SGD com momentum para exploração robusta
# - LR mais alto (necessário para SGD)
# - Batch pequeno para regularização implícita
# - Mais épocas para compensar convergência mais lenta
############################################################

############################
# METADADOS DO EXPERIMENTO
############################
[experiment]
name = bertpli3
# name = bertpli_sgd_lr1e4_bs8_ep6

description = |
    SGD com momentum oferece trajetória diferente no espaço de loss.
    Batch size pequeno (8) adiciona ruído estocástico benéfico.
    LR 1e-4 apropriado para SGD em fine-tuning BERT.
    6 épocas para convergência completa.
seed = 42

############################
# AMBIENTE DE EXECUÇÃO
############################
[environment]
precision = fp16
notes = |
    SGD pode ser mais lento mas frequentemente generaliza melhor.
    Batch size 8 reduz requisitos de memória (~8GB VRAM).

############################
# DADOS
############################
[data]
data_dir = data/
max_paragraphs = 64
max_seq_length = 256
train_dataset_type = JsonFromFiles
train_formatter_type = BertPairText
train_data_path = data
train_file_list = train_task2.json
valid_dataset_type = JsonFromFiles
valid_formatter_type = BertPairText
valid_data_path = data
valid_file_list = valid_task2.json
test_dataset_type = JsonFromFiles
test_formatter_type = BertPairText
test_data_path = data
test_file_list = test_task1.json
recursive = False
json_format = line
load_into_mem = True

############################
# MODELO
############################
[model]
pretrained_model = bert-base-uncased
aggregation_model = atten_rnn
hidden_size = 256
dropout = 0.2
freeze_bert = false
freeze_layers = 0
model_name = BertPoint
bert_path = bert-base-uncased
output_dim = 2
output_mode = classification

############################
# TREINAMENTO
############################
[train]
epoch = 6
batch_size = 8
shuffle = true
gradient_accumulation_steps = 2
optimizer = sgd
learning_rate = 1e-4
weight_decay = 0.001
max_grad_norm = 1.0
warmup_ratio = 0.15
lr_scheduler = cosine
early_stopping = true
early_stopping_patience = 3
reader_num = 0
step_size = 1
lr_multiplier = 1

############################
# AVALIAÇÃO
############################
[eval]
metrics = precision, recall, f1
eval_strategy = epoch
monitor_metric = f1
monitor_mode = max
# pool_out=True: estágio 1 (extrator de embeddings). Avaliação de recuperação é do AttenRNN.
run_test_at_end = false
eval_batch_size = 8
eval_reader_num = 0

############################
# LOGGING & OUTPUT
############################
[logging]
output_dir = output/experiment
save_checkpoints = true
save_strategy = epoch
log_loss = true
log_metrics = true
logger = tensorboard

############################
# OUTPUT
############################
[output]
output_time = 1
test_time = 1
model_path = output/checkpoints
model_name = bert_finetuned
pool_out = True
save_as_dict = True
tensorboard_path = output/tensorboard
accuracy_method = SingleLabelTop1
output_function = Basic
output_value = micro_precision,macro_precision,macro_recall,macro_f1
tqdm_ncols = 150

############################
# MONITORAMENTO DE RECURSOS
############################
[monitoring]
enable_monitoring = true
collect_energy = true
collect_ram = true
collect_flops = true
collect_time = true
energy_tool = codecarbon
ram_tool = psutil
flops_tool = torch_profiler
monitoring_granularity = epoch
metrics_output_dir = outputs/metrics/
save_summary = true