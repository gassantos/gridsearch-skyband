############################################################
# EXPERIMENTO BERT-PLI - Configuração 2
# Objetivo: Recuperação de Casos Jurídicos em Nível de Parágrafo
#
# Estratégia:
# - Adam clássico com LR moderado
# - Batch size maior para estabilidade
# - Mais épocas para convergência gradual
############################################################

############################
# METADADOS DO EXPERIMENTO
############################
[experiment]
name = bertpli2
# name = bertpli_adam_lr3e5_bs32_ep5

description = |
    Configuração com Adam clássico e batch maior.
    LR ligeiramente aumentado (3e-5) para acelerar aprendizado inicial.
    5 épocas permitem explorar melhor o espaço de parâmetros.
    Adequado para datasets médios/grandes.
seed = 42

############################
# AMBIENTE DE EXECUÇÃO
############################
[environment]
precision = fp16
notes = |
    Batch size 32 requer ~16GB VRAM.
    Considerar gradient accumulation se necessário.

############################
# DADOS
############################
[data]
data_dir = data/
max_paragraphs = 64
max_seq_length = 256
train_dataset_type = JsonFromFiles
train_formatter_type = BertPairText
train_data_path = data
train_file_list = train_task2.json
valid_dataset_type = JsonFromFiles
valid_formatter_type = BertPairText
valid_data_path = data
valid_file_list = valid_task2.json
test_dataset_type = JsonFromFiles
test_formatter_type = BertPairText
test_data_path = data
test_file_list = test_task1.json
recursive = False
json_format = line
load_into_mem = True

############################
# MODELO
############################
[model]
pretrained_model = bert-base-uncased
aggregation_model = atten_rnn
hidden_size = 256
dropout = 0.15
freeze_bert = false
freeze_layers = 0
model_name = BertPoint
bert_path = bert-base-uncased
output_dim = 2
output_mode = classification

############################
# TREINAMENTO
############################
[train]
epoch = 5
batch_size = 32
shuffle = true
gradient_accumulation_steps = 1
optimizer = adam
learning_rate = 3e-5
weight_decay = 0.0
max_grad_norm = 1.0
warmup_ratio = 0.1
lr_scheduler = linear
early_stopping = true
early_stopping_patience = 3
reader_num = 0
step_size = 1
lr_multiplier = 1

############################
# AVALIAÇÃO
############################
[eval]
metrics = precision, recall, f1
eval_strategy = epoch
monitor_metric = f1
monitor_mode = max
# pool_out=True: estágio 1 (extrator de embeddings). Avaliação de recuperação é do AttenRNN.
run_test_at_end = false
eval_batch_size = 16
eval_reader_num = 0

############################
# LOGGING & OUTPUT
############################
[logging]
output_dir = output/experiment
save_checkpoints = true
save_strategy = epoch
log_loss = true
log_metrics = true
logger = tensorboard

############################
# OUTPUT
############################
[output]
output_time = 1
test_time = 1
model_path = output/checkpoints
model_name = bert_finetuned
pool_out = True
save_as_dict = True
tensorboard_path = output/tensorboard
accuracy_method = SingleLabelTop1
output_function = Basic
output_value = micro_precision,macro_precision,macro_recall,macro_f1
tqdm_ncols = 150

############################
# MONITORAMENTO DE RECURSOS
############################
[monitoring]
enable_monitoring = true
collect_energy = true
collect_ram = true
collect_flops = true
collect_time = true
energy_tool = codecarbon
ram_tool = psutil
flops_tool = torch_profiler
monitoring_granularity = epoch
metrics_output_dir = outputs/metrics/
save_summary = true